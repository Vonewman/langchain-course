{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f692702c-6a3d-477e-b3a1-2faf93dd0e3a",
   "metadata": {},
   "source": [
    "# Quick Intro to Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9c1099-d46c-4985-9221-619438e320eb",
   "metadata": {},
   "source": [
    "## LLMs in general"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c081aaa9-93bf-4b29-8601-11387657032a",
   "metadata": {},
   "source": [
    "### Tokens Distributions and Predicting the Next Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "240b8199-799b-4041-ab4a-646fc30c3f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Rainbow Socks Co.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad04f8a6-66e1-4474-9147-a6d39b4124ca",
   "metadata": {},
   "source": [
    "### Tracking Token Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27617511-0a0e-42e3-8433-38ed71a30989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 44\n",
      "\tPrompt Tokens: 4\n",
      "\tCompletion Tokens: 40\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00088\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", n=2, best_of=2)\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm(\"Tell me a joke\")\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b479abc1-4465-482f-9f04-326c50d297d5",
   "metadata": {},
   "source": [
    "### Few-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "109fcef1-145c-4e9f-b58a-0ec5dcba4629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"What's the weather like?\",\n",
    "        \"answer\": \"It's raining cats and dogs, better bring an umbrella!\"\n",
    "    }, {\n",
    "        \"query\": \"How old are you?\",\n",
    "        \"answer\": \"Age is just a number, but I'm timeless.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cb9b0f3-3d85-4532-8d6d-403829ebcc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
    "assistant. The assistant is known for its humor and wit, providing\n",
    "entertaining and amusing responses to users' questions. Here are some\n",
    "examples:\n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# now create the few-shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be3c1b98-503b-4770-9b5e-b3d3526c907e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To find the perfect balance between pizza and ice cream.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "\n",
    "# load the model\n",
    "chat = ChatOpenAI(temperature=0.0)\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=few_shot_prompt_template)\n",
    "chain.run(\"What's the meaning of life?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afbaeef-2379-4788-9116-61e331ca8fae",
   "metadata": {},
   "source": [
    "### Creating a Question-Answering Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4312aee-8a76-47b1-9f2b-655c0680347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(\n",
    "        template=template,\n",
    "    input_variables=['question']\n",
    ")\n",
    "\n",
    "# user question\n",
    "question = \"What is the capital city of France?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0d86a2f-1565-484c-b95f-6a083451f7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cf059fd-2f40-4085-af36-98c164d6b9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adiallo/Desktop/Course-on-Langchain/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub, LLMChain\n",
    "\n",
    "# initialize Hub LLM\n",
    "hub_llm = HuggingFaceHub(\n",
    "        repo_id='google/flan-t5-large',\n",
    "    model_kwargs={'temperature':0},\n",
    ")\n",
    "\n",
    "# create prompt template > LLM chain\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=hub_llm\n",
    ")\n",
    "\n",
    "# ask the user question about the capital of France\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5338cab-762a-4383-a9ba-9356300a0937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='paris', generation_info=None)], [Generation(text='giraffe', generation_info=None)], [Generation(text='nitrogen', generation_info=None)], [Generation(text='yellow', generation_info=None)]] llm_output=None run=RunInfo(run_id=UUID('4e49df0b-baa6-4ba2-b627-422ac09ddc83'))\n"
     ]
    }
   ],
   "source": [
    "qa = [\n",
    "    {'question': \"What is the capital city of France?\"},\n",
    "    {'question': \"What is the largest mammal on Earth?\"},\n",
    "    {'question': \"Which gas is most abundant in Earth's atmosphere?\"},\n",
    "    {'question': \"What color is a ripe banana?\"}\n",
    "]\n",
    "res = llm_chain.generate(qa)\n",
    "print( res )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dd20a0d-5285-4ea4-9477-4d2f0481ac8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris\\nBlue Whale\\nNitrogen\\nYellow'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "long_prompt = PromptTemplate(template=multi_template, input_variables=[\"questions\"])\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "qs_str = (\n",
    "    \"What is the capital city of France?\\n\" +\n",
    "    \"What is the largest mammal on Earth?\\n\" +\n",
    "    \"Which gas is most abundant in Earth's atmosphere?\\n\" +\n",
    "\t\t\"What color is a ripe banana?\\n\"\n",
    ")\n",
    "llm_chain.run(qs_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c58089f-76f9-4808-ba0f-c451542723cc",
   "metadata": {},
   "source": [
    "### Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55b9d64d-4144-4e97-8659-de8e708fc8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7340944a-ec06-4b8b-bb83-a17a6cb51595",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_template = \"Summarize the following text to one sentence: {text}\"\n",
    "summarization_prompt = PromptTemplate(input_variables=[\"text\"], template=summarization_template)\n",
    "summarization_chain = LLMChain(llm=llm, prompt=summarization_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3d086cb-bdb2-4b59-895f-c5f40741b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"LangChain provides many modules that can be used to build language model applications. Modules can be combined to create more complex applications, or be used individually for simple applications. The most basic building block of LangChain is calling an LLM on some input. Let’s walk through a simple example of how to do this. For this purpose, let’s pretend we are building a service that generates a company name based on what the company makes.\"\n",
    "summarized_text = summarization_chain.predict(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4dea006-593a-4ccd-8451-1474544b1613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain offers various modules for building language model applications, allowing users to combine them for more complex applications or use them individually for simpler ones, with the basic building block being calling an LLM on input, as demonstrated in the example of creating a company name based on its product.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1254bf85-ba8f-419c-8d77-3f5e0aa5eec1",
   "metadata": {},
   "source": [
    "### Text Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2171ff7d-c6f0-4cee-8909-97f2038a4184",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_template = \"Translate the following text from {source_language} to {target_language}: {text}\"\n",
    "translation_prompt = PromptTemplate(input_variables=[\"source_language\", \"target_language\", \"text\"], template=translation_template)\n",
    "translation_chain = LLMChain(llm=llm, prompt=translation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9793aec-65c4-4b6b-b6fc-b58ba76c83a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_language = \"English\"\n",
    "target_language = \"French\"\n",
    "text = \"I hate you for this.\"\n",
    "translated_text = translation_chain.predict(source_language=source_language, target_language=target_language, text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42b55a9c-1538-497b-961e-009d9e522c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Je te déteste pour ça.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06435aac-11ae-4d39-be49-0ee06ab04d16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
